---
layout: page
title: A Sample Application
subtitle: Finally?
---

You will want to set up an Anaconda environment or make sure that you have matplotlib installed.

You can then enter commands into the cells directly to Spark, save, edit, and update your instance in real time. Once you have it working the way you want, you can even download it as a .py file, or save it, or copy it into another notebook. I download a different textfile that was a little larger for this sample: [hamilton-federalist-548.txt](http://www.textfiles.com/etext/NONFICTION/hamilton-federalist-548.txt)

![SparkShell](../img/Sample1.png){:class="img-responsive"}
![SparkShell](../img/Sample2.png){:class="img-responsive"}
![SparkShell](../img/Sample3.png){:class="img-responsive"}
![SparkShell](../img/Sample4.png){:class="img-responsive"}

There are sample applications included with the Spark installation in the examples folder. Running examples in an interactive setting like that provided by Jupyter and IPython should increase understanding of working with the data and speed learning time.

We have barely scratched the surface of Spark, RDD's, actions, and transformations, let alone distributing Spark across multiple nodes. Amazon Web Services provides the ability to integrate Spark on Elastic map Reduce: [AWS Spark](https://aws.amazon.com/emr/details/spark/) and Google provides the managed Spark and Hadoop in the cloud through [Cloud DataProc](https://cloud.google.com/dataproc/).

Hopefully by being able to work interactively with data locally with Spark, when the time comes to deploy a user won't have to worry as much about configuration because the service providers can assist with that, but instead the developer can focus on analyzing their data to extract the most value. 
